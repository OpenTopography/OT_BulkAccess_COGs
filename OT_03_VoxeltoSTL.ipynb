{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-night",
   "metadata": {},
   "source": [
    "# Subsampling Lidar Point Clouds for STL Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-commissioner",
   "metadata": {},
   "source": [
    "## Authors\n",
    "Author1 = {\"name\": \"Nathaniel Quinn\", \"affiliation\": \"UNAVCO, Inc.\", \"email\": \"nathaniel.quinn@unavco.org\", \"orcid\":\"\"}\n",
    "\n",
    "## Purpose\n",
    "Subsampling Voxel method for point cloud data thinning. \n",
    "Method to balance computation speeds with model accuracy.\n",
    "\n",
    "After point cloud has been thinned, triangulation is computed to create a mesh. This mesh can be exported as a STL file and opened in a variety of 3D modeling software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-cooking",
   "metadata": {},
   "source": [
    "## Funding\n",
    "OpenTopography is supported by the National Science Foundation under Award Numbers 1948997, 1948994 & 1948857\n",
    "\n",
    "## Acknowledgments\n",
    "This notebook is adapted from work by Florent Poux. For more details see his [medium blogpost](https://towardsdatascience.com/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-humidity",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries: NumPy,LasPy, Numpy-STL.  Note: We are using las files, which can be downloaded from OpenTopography.org for your specified region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prescription-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laspy as lp\n",
    "from stl import mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-jason",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Load point cloud data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fundamental-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_file = \"points.las\"\n",
    "\n",
    "point_cloud = lp.read(lidar_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-anaheim",
   "metadata": {},
   "source": [
    "Use LasPy library to get X,Y, and Z coordinates, then place them into NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advance-nepal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in original file: 3573262\n"
     ]
    }
   ],
   "source": [
    "#store coordinates in \"points\"\n",
    "points = np.vstack((point_cloud.x, point_cloud.y, point_cloud.z)).transpose()\n",
    "print(\"Number of points in original file:\",len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-contrast",
   "metadata": {},
   "source": [
    "**Grid Sampling of Point Cloud**\n",
    "\n",
    "Using cubic cells called voxels, for each voxel, we use one representative point.  This point may be the barycenter of the points in the cell, or the point that is closest to the barycenter.\n",
    "\n",
    "Change the \"voxel_size\" to vary subset resolution. Recommend somewhere between 5 and 15 for most OpenTopography datasets.\n",
    "\n",
    "Note: voxel_size is in Meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saved-cricket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The voxel grid is X,Y,Z voxels: [109.  87.  16.]\n"
     ]
    }
   ],
   "source": [
    "voxel_size=15\n",
    "nb_vox=np.ceil((np.max(points, axis=0) - np.min(points, axis=0))/voxel_size)\n",
    "print(\"The voxel grid is X,Y,Z voxels:\", (nb_vox))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-macintosh",
   "metadata": {},
   "source": [
    "Let's get a sense of how much the point cloud data will be reduced, both by point number and percentage of overall data subset into this new dataset.\n",
    "\n",
    "NOTE: If the subset percentage is negative, increase your voxel size as your voxel is smaller than collected data resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominican-stress",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will reduce number of points to 151728\n",
      "Or reduce by 95.753795831372 %\n"
     ]
    }
   ],
   "source": [
    "nb_vox_readout = np.prod(nb_vox, dtype=int) \n",
    "print(\"This will reduce number of points to\", nb_vox_readout)\n",
    "\n",
    "pts_length = len(points)\n",
    "perct = ((1-(nb_vox_readout/pts_length))*100)\n",
    "print(\"Or reduce by\", perct, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-insight",
   "metadata": {},
   "source": [
    "**Define the function.**\n",
    "\n",
    "Within this function, each voxel is tested to see if it contains one or more points.  Voxels that have points are kept in the set, and empty voxels are indexed. It then computes the point data that will be used to represent each individual voxel. \n",
    "\n",
    "Function credit: Florent Poux.  For more details see his [medium blogpost](https://towardsdatascience.com/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that takes as input an array of points, and a voxel size expressed in meters. It returns the sampled point cloud\n",
    "def grid_subsampling(points, voxel_size):\n",
    "\n",
    "  nb_vox=np.ceil((np.max(points, axis=0) - np.min(points, axis=0))/voxel_size)\n",
    "  non_empty_voxel_keys, inverse, nb_pts_per_voxel= np.unique(((points - np.min(points, axis=0)) // voxel_size).astype(int), axis=0, return_inverse=True, return_counts=True)\n",
    "  idx_pts_vox_sorted=np.argsort(inverse)\n",
    "  voxel_grid={}\n",
    "  grid_barycenter,grid_candidate_center=[],[]\n",
    "  last_seen=0\n",
    "\n",
    "  for idx,vox in enumerate(non_empty_voxel_keys):\n",
    "    voxel_grid[tuple(vox)]=points[idx_pts_vox_sorted[last_seen:last_seen+nb_pts_per_voxel[idx]]]\n",
    "    grid_barycenter.append(np.mean(voxel_grid[tuple(vox)],axis=0))\n",
    "    grid_candidate_center.append(voxel_grid[tuple(vox)][np.linalg.norm(voxel_grid[tuple(vox)]-np.mean(voxel_grid[tuple(vox)],axis=0),axis=1).argmin()])\n",
    "    last_seen+=nb_pts_per_voxel[idx]\n",
    "\n",
    "  return grid_candidate_center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-combining",
   "metadata": {},
   "source": [
    "Execute the function with the original point cloud data set and voxel size established previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "silver-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sampled_point_cloud = grid_subsampling(points, voxel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-wells",
   "metadata": {},
   "source": [
    "Use numpy to convert list into an array for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naval-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sample_pc_np = np.array(grid_sampled_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-parallel",
   "metadata": {},
   "source": [
    "**Subsampled Point Cloud Plotting**\n",
    "\n",
    "Plot just the points to get a sense of point density. Adjust voxel size depending on needs, balancing between accuracy, STL file size, and rendering speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-imaging",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-mississippi",
   "metadata": {},
   "source": [
    "To view all points in subset dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-evidence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 10), dpi=150)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(grid_sample_pc_np[:,0], grid_sample_pc_np[:,1], grid_sample_pc_np[:,2], s = 0.05, c = 'black')\n",
    "ax.view_init(60, 35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-seminar",
   "metadata": {},
   "source": [
    "**Triangulating for surface output**\n",
    "\n",
    "Create a surface by triangulating between every 3 adjacent points.  This type of surfacing may be best for natural features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 10), dpi=150)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "trisurf = ax.plot_trisurf(grid_sample_pc_np[:,0], grid_sample_pc_np[:,1], grid_sample_pc_np[:,2], \n",
    "                ax.view_init(60, 35), cmap='viridis', edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-radiation",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "To export mesh as STL file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "guilty-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.tri as mtri\n",
    "\n",
    "x_all = grid_sample_pc_np[:,0]\n",
    "y_all = grid_sample_pc_np[:,1]\n",
    "z_all = grid_sample_pc_np[:,2]\n",
    "\n",
    "tris = mtri.Triangulation(x_all, y_all)\n",
    "\n",
    "data = np.zeros(len(tris.triangles), dtype=mesh.Mesh.dtype)\n",
    "m = mesh.Mesh(data, remove_empty_areas=False)\n",
    "m.x[:] = x_all[tris.triangles]\n",
    "m.y[:] = y_all[tris.triangles]\n",
    "m.z[:] = z_all[tris.triangles]\n",
    "\n",
    "m.save('point_cloud_model3D_1.stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-lewis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
